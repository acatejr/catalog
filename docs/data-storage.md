---
title: Data Storage and Database Schema
description: PostgreSQL vector database implementation for storing metadata chunks with embeddings for RAG-based semantic search
tags:
  - database
  - postgresql
  - vector-database
  - pgvector
  - embeddings
  - schema
  - storage
  - rag
---

# Data Storage

## Overview

The Catalog application stores metadata in a PostgreSQL database with vector extension support, enabling semantic search through vector embeddings. Metadata harvested from various sources is processed, chunked, embedded, and stored in a single `documents` table optimized for Retrieval-Augmented Generation (RAG) queries.

## Database Schema

### Documents Table

The `documents` table is defined in `schema.sql:4-17` and contains the following structure:

```sql
CREATE table if not exists documents (
    id SERIAL PRIMARY KEY,
    title TEXT,
    description TEXT,
    keywords TEXT[],
    authors TEXT[],
    chunk_text TEXT,
    chunk_index INTEGER,
    embedding vector(384),
    created_at TIMESTAMP DEFAULT NOW(),
    doc_id varchar(255),
    chunk_type varchar(255),
    data_source varchar(75)
);
```

#### Column Descriptions

| Column | Type | Description |
|--------|------|-------------|
| `id` | SERIAL | Auto-incrementing primary key |
| `title` | TEXT | Title of the source document |
| `description` | TEXT | Full description or abstract from the source |
| `keywords` | TEXT[] | Array of keywords/tags associated with the document |
| `authors` | TEXT[] | Array of author names (currently unused) |
| `chunk_text` | TEXT | The actual text chunk that was embedded |
| `chunk_index` | INTEGER | Sequential index of the chunk within the parent document |
| `embedding` | vector(384) | 384-dimensional vector embedding generated by `all-MiniLM-L6-v2` |
| `created_at` | TIMESTAMP | Timestamp when the record was created |
| `doc_id` | varchar(255) | SHA-256 hash of the original document title (used for deduplication) |
| `chunk_type` | varchar(255) | Type of chunk (e.g., "title+description+keywords") |
| `data_source` | varchar(75) | Source identifier: "fsgeodata", "datahub", or "rda" |

## Data Model

The application uses Pydantic models to validate and structure metadata before storage. The core model is defined in `schema.py:11-27`:

```python
class USFSDocument(BaseModel):
    id: str
    title: str
    description: str
    keywords: Optional[List[str]] = None
    src: Optional[str] = None
```

This model represents a unified structure for metadata from all three data sources.

## Storage Process

### 1. Metadata Collection and Parsing

Metadata is harvested from three sources, each producing a standardized dictionary structure:

#### Source Format Mapping

| Source | Format | Parser Function | Fields Extracted |
|--------|--------|----------------|------------------|
| FS GeoData | XML | `_parse_fsgeodata_metadata()` (src/cli.py) | title, abstract, keywords, metadata_source_url |
| DataHub | JSON | `_parse_datahub_metadata()` (src/cli.py) | title, description, keywords, identifier, url |
| RDA | JSON | `_parse_rda_metadata()` (src/cli.py) | title, description, keywords, identifier, url |

Each parser generates a dictionary with these common fields:
- `id`: SHA-256 hash of the lowercase title (for deduplication)
- `title`: Document title
- `description`: Document description or abstract
- `keywords`: List of keyword strings
- `src`: Source identifier ("fsgeodata", "datahub", or "rda")

### 2. Document Merging

All parsed metadata is merged into a single list using the `merge_docs()` function (src/cli.py:59-79), which:
- Combines documents from all sources
- Removes duplicates based on the `id` field
- Ensures unique documents across all data sources

### 3. Embedding and Storage

The `embed_and_store()` command (src/cli.py:280-320) performs the final processing:

#### Text Chunking

Each document's text is combined and chunked:

```python
combined_text = f"Title: {title}\nDescription: {description}\nKeywords: {keywords}\nSource: {src}"
```

The `RecursiveCharacterTextSplitter` (from LangChain) splits this text into 65-character chunks with no overlap, creating multiple records per document if the combined text exceeds 65 characters.

#### Vector Embedding

Each chunk is embedded using the `all-MiniLM-L6-v2` sentence transformer model, producing a 384-dimensional vector that captures semantic meaning.

#### Database Insertion

The `save_to_vector_db()` function (src/db.py) inserts each chunk into the `documents` table with:
- The embedding vector
- Chunk metadata (doc_id, chunk_index, chunk_type, chunk_text)
- Original document metadata (title, description, keywords, data_source)

Duplicate insertions are handled by catching `UniqueViolation` errors and rolling back the transaction.

## Storage Operations

### Insert Document Chunk

```python
save_to_vector_db(embedding, metadata, title, desc)
```

Located in `db.py:71-98`. Inserts a single document chunk with its embedding and metadata.

### Empty Documents Table

```python
empty_documents_table()
```

Located in `src/db.py`. Deletes all records from the `documents` table and performs a `VACUUM FULL` to reclaim storage space.

### Count Documents

```python
count_documents()
```

Located in `src/db.py`. Returns the total number of records in the `documents` table.

### Search Documents

```python
search_docs(query_embedding, limit=10)
```

Located in `src/db.py`. Performs vector similarity search using cosine distance:

```sql
SELECT id, title, description, keywords,
       1 - (embedding <=> %s::vector) AS similarity_score
FROM documents
WHERE embedding IS NOT NULL
ORDER BY embedding <=> %s::vector
LIMIT %s;
```

Returns documents ranked by similarity score (higher = more similar).

## CLI Commands

### Download All Metadata

```bash
PYTHONPATH=src python src/cli.py download-all
```

Downloads metadata from all three sources (FSGeoData, DataHub, RDA).

### Embed and Store

```bash
PYTHONPATH=src python src/cli.py embed-and-store
```

Processes all parsed metadata, generates embeddings, and stores chunks in the database.

### Clear Documents Table

```bash
PYTHONPATH=src python src/cli.py clear-docs-table
```

Empties the `documents` table completely.

### Run API

```bash
PYTHONPATH=src python src/cli.py run-api
```

Starts the FastAPI server for querying the catalog.

## Example Data Flow

1. **Harvest**: Download XML/JSON metadata from sources
2. **Parse**: Extract title, description, keywords into standardized dictionaries
3. **Merge**: Combine and deduplicate documents from all sources
4. **Chunk**: Split combined text into 65-character chunks
5. **Embed**: Generate 384-dimensional vector for each chunk using `all-MiniLM-L6-v2`
6. **Store**: Insert chunk records into PostgreSQL with vector embeddings

## Storage Characteristics

- **Chunking Strategy**: 65-character chunks with no overlap
- **Embedding Model**: `all-MiniLM-L6-v2` (384 dimensions)
- **Deduplication**: SHA-256 hash of lowercase title as `doc_id`
- **Search Method**: Cosine similarity on vector embeddings
- **Vector Extension**: PostgreSQL `pgvector` extension

## Notes

- The `authors` column is defined in the schema but currently unused by the application
- Each source document may generate multiple rows in the `documents` table (one per chunk)
- The `chunk_index` field allows reconstruction of the original document order if needed
- Vector search performance can be improved with indexing (e.g., HNSW or IVFFlat indexes)
