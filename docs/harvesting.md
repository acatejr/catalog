---
title: Harvesting the Metadata
---

# Data Harvesting

Timbercat harvests metadata from three different data sources. Each source has its own loader class that handles downloading and parsing.

## The Three Harvesters

### 1. FSGeodata Clearinghouse (`fsgeodata.py`)

The most complex harvester. It scrapes an HTML datasets page to find metadata and service URLs.

**What it harvests:**

- XML metadata files (title, abstract, keywords, lineage)
- MapServer JSON service definitions

**How it works:**

```python
class FSGeodataLoader:
    def download_all(self):
        # 1. Fetch the datasets page
        html_content = self.fetch_datasets_page()

        # 2. Parse HTML with BeautifulSoup to find links
        datasets = self.parse_datasets(html_content)

        # 3. Download each XML and JSON file
        for dataset in datasets:
            self.download_file(dataset["metadata_url"], ...)
            self.download_service_info(dataset["service_url"], ...)
```

**Output:**

- `data/fsgeodata/metadata/*.xml`
- `data/fsgeodata/services/*_service.json`

### 2. Geospatial Data Discovery (`gdd.py`)

The simplest harvester. Downloads a single JSON file from an ArcGIS Hub API endpoint.

**What it harvests:**

- DCAT-US formatted JSON catalog (title, description, keywords, themes)

**How it works:**

```python
class GeospatialDataDiscovery:
    def download_gdd_metadata(self):
        # Single API call to get all datasets
        response = requests.get(METADATA_SOURCE_URL)
        # Save entire catalog as one JSON file
```

**Output:**

- `data/gdd/gdd_metadata.json`

### 3. Research Data Archive (`rda.py`)

Similar to GDD. Downloads a single JSON catalog file from a web service.

**What it harvests:**

- JSON catalog (title, description, keywords)

**How it works:**

```python
class RDALoader:
    def download(self):
        # Single API call
        response = requests.get(SOURCE_URL)
        json_data = response.json()
```

**Output:**

- `data/rda/rda_metadata.json`

## Common Pattern

All three harvesters follow the same two-phase pattern:

```mermaid
graph LR
    A[Download] --> B[Parse]
    B --> C[Vector DB]
```

1. **Download Phase**: Fetch raw data (XML or JSON)
2. **Parse Phase**: Extract fields and create standardized document objects

Each parser creates documents with a common structure:

```python
document = {
    "id": hash_string(title),
    "title": title,
    "description": description,  # or "abstract" for XML
    "keywords": keywords,
    "src": "gdd"  # or "rda" or "fsgeodata"
}
```

## Building the Compiled Catalog

After downloading and parsing from all three sources, `build_docs_catalog()` creates a single unified catalog file.

**What it does:**

```python
def build_docs_catalog():
    # 1. Create instances of all three loaders
    fsgeod = FSGeodataLoader()
    rda = RDALoader()
    gdd = GeospatialDataDiscovery()

    # 2. Parse metadata from each source
    fsgeo_docs = fsgeod.parse_metadata()
    rda_docs = rda.parse_metadata()
    gdd_docs = gdd.parse_metadata()

    # 3. Combine all documents into one list
    documents = fsgeo_docs + rda_docs + gdd_docs

    # 4. Save to a single JSON file
    save_json(documents, "data/catalog.json")
```

**Output:**

- `data/catalog.json` - A single file containing all parsed documents from all three sources

This compiled catalog serves as the input for loading into the vector database. Instead of reading hundreds of individual XML files and multiple JSON files, the vector database loader can simply read one consolidated file.

**Why this matters:**

- **Single source of truth**: One file to maintain and version
- **Faster loading**: Vector DB can read one file instead of hundreds
- **Easy inspection**: You can examine the entire catalog structure in one place

**Running it:**

```bash
# Build the compiled catalog from all sources
timbercat build-docs-catalog
```

The command reports how many documents were parsed from each source and the total count.

## Key Design Decisions

**Source Identification**: Each document includes a `src` field to track which system it came from.

**ID Generation**: Document IDs are generated by hashing the title, ensuring uniqueness and consistency.

**Error Handling**: All harvesters handle missing files and failed requests gracefully.

**Rate Limiting**: FSGeodata includes a 0.5s delay between requests to be respectful of server resources.

## Running the Harvesters

```bash
# Download from Clearinghouse (XML + JSON)
timbercat download-fsgeodata

# Other sources can be added to the CLI similarly
```

## Challenges

**Different Data Formats**: XML (FSGeodata) vs JSON (GDD, RDA) require different parsing strategies.

**Inconsistent Schemas**: Not all XML metadata files have the same structure. Defensive parsing with existence checks is essential.

**Missing Services**: Some FSGeodata datasets don't have MapServer services. The harvester handles these gracefully.

## Next Steps

After harvesting, the parsed documents are loaded into ChromaDB for semantic search. See the implementation in `src/catalog/` for the full loader classes.