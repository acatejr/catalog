{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Catalog","text":"<p>Catalog is a Python CLI that automates discovery and understanding of Government Agency geospatial and tabular data. It harvests XML metadata and MapServer service definitions from three anonymized portals\u2014Research Archive (RA), Geospatial Discovery (GD), and Agency Geodata Portal (AGP)\u2014builds embeddings, and lets you explore datasets with a semantic, RAG-powered search so you can answer questions like \u201cwhat data exists and how do I use it?\u201d without manual spelunking.</p>"},{"location":"#why-it-matters","title":"Why it matters","text":"<ul> <li>Hunting across portals, downloading XML one-by-one, and reconciling service URLs slows research and product teams.</li> <li>Explaining lineage and \u201cfit for purpose\u201d to stakeholders is hard without a unified view.</li> <li>Traditional keyword search misses nuance; semantic search with LLMs surfaces relevant datasets faster.</li> </ul>"},{"location":"#what-catalog-does","title":"What Catalog does","text":"<ul> <li>Automated harvesting from RA, GD, and AGP (XML + MapServer JSON).</li> <li>Embeds metadata with a vector database (table) and uses LLMs in a Retrieval-Augmented Generation (RAG) flow for semantic Q&amp;A.</li> <li>Python Click-based CLI (<code>timbercat</code>) to harvest, inspect, and query datasets.</li> <li>Outputs organized metadata and service URLs you can plug into dashboards or analyses.</li> </ul>"},{"location":"#the-process-at-a-glance","title":"The process (at a glance)","text":"<pre><code>flowchart TB\n  Sources[RA / GD / AGP] --&gt; Harvester[timbercat harvest]\n  Harvester --&gt; Normalize[\"Normalize metadata (XML + JSON)\"]\n  Normalize --&gt; Embed[VectorDB embeddings]\n  Embed --&gt; RAG[LLM + RAG pipeline]\n  RAG --&gt; Answers[Semantic search &amp; dataset guidance]\n</code></pre> <ol> <li>Identify metadata sources:  </li> <li>Research Archive (RA): research-grade datasets from the agency's research directorate and partners.  </li> <li>Geospatial Discovery (GD): current operational GIS layers and services.  </li> <li> <p>Agency Geodata Portal (AGP): authoritative basemaps, boundaries, operational layers, and raster products.</p> </li> <li> <p>Harvest metadata: <code>timbercat harvest</code> pulls XML and MapServer JSON, normalizes fields, and stores them for indexing.</p> </li> <li> <p>Build the vector database: embeddings go into vector storage; metadata stays linked for provenance.</p> </li> <li> <p>RAG-based search: the CLI uses the embeddings plus an LLM to answer dataset and lineage questions with grounded citations.</p> </li> </ol>"},{"location":"#where-to-go-next","title":"Where to go next","text":"<ul> <li>Architecture and design decisions: see <code>docs/architecture.md</code>.</li> <li>Data sources deep dive: see <code>docs/data-sources.md</code>.</li> <li>CLI usage and examples: see <code>docs/cli.md</code>.</li> <li>Vector DB details and comparisons: see <code>docs/vector-db.md</code>.</li> </ul>"},{"location":"cli/","title":"Command Line Interface (CLI)","text":"<p>The Command Line Interface (CLI)</p>"},{"location":"harvesting/","title":"Data Harvesting","text":"<p>Timbercat harvests metadata from three different data sources. Each source has its own loader class that handles downloading and parsing.</p>"},{"location":"harvesting/#the-three-harvesters","title":"The Three Harvesters","text":""},{"location":"harvesting/#1-fsgeodata-clearinghouse-fsgeodatapy","title":"1. FSGeodata Clearinghouse (<code>fsgeodata.py</code>)","text":"<p>The most complex harvester. It scrapes an HTML datasets page to find metadata and service URLs.</p> <p>What it harvests:</p> <ul> <li>XML metadata files (title, abstract, keywords, lineage)</li> <li>MapServer JSON service definitions</li> </ul> <p>How it works:</p> <pre><code>class FSGeodataLoader:\n    def download_all(self):\n        # 1. Fetch the datasets page\n        html_content = self.fetch_datasets_page()\n\n        # 2. Parse HTML with BeautifulSoup to find links\n        datasets = self.parse_datasets(html_content)\n\n        # 3. Download each XML and JSON file\n        for dataset in datasets:\n            self.download_file(dataset[\"metadata_url\"], ...)\n            self.download_service_info(dataset[\"service_url\"], ...)\n</code></pre> <p>Output:</p> <ul> <li><code>data/fsgeodata/metadata/*.xml</code></li> <li><code>data/fsgeodata/services/*_service.json</code></li> </ul>"},{"location":"harvesting/#2-geospatial-data-discovery-gddpy","title":"2. Geospatial Data Discovery (<code>gdd.py</code>)","text":"<p>The simplest harvester. Downloads a single JSON file from an ArcGIS Hub API endpoint.</p> <p>What it harvests:</p> <ul> <li>DCAT-US formatted JSON catalog (title, description, keywords, themes)</li> </ul> <p>How it works:</p> <pre><code>class GeospatialDataDiscovery:\n    def download_gdd_metadata(self):\n        # Single API call to get all datasets\n        response = requests.get(METADATA_SOURCE_URL)\n        # Save entire catalog as one JSON file\n</code></pre> <p>Output:</p> <ul> <li><code>data/gdd/gdd_metadata.json</code></li> </ul>"},{"location":"harvesting/#3-research-data-archive-rdapy","title":"3. Research Data Archive (<code>rda.py</code>)","text":"<p>Similar to GDD. Downloads a single JSON catalog file from a web service.</p> <p>What it harvests:</p> <ul> <li>JSON catalog (title, description, keywords)</li> </ul> <p>How it works:</p> <pre><code>class RDALoader:\n    def download(self):\n        # Single API call\n        response = requests.get(SOURCE_URL)\n        json_data = response.json()\n</code></pre> <p>Output:</p> <ul> <li><code>data/rda/rda_metadata.json</code></li> </ul>"},{"location":"harvesting/#common-pattern","title":"Common Pattern","text":"<p>All three harvesters follow the same two-phase pattern:</p> <pre><code>graph LR\n    A[Download] --&gt; B[Parse]\n    B --&gt; C[Vector DB]\n</code></pre> <ol> <li>Download Phase: Fetch raw data (XML or JSON)</li> <li>Parse Phase: Extract fields and create standardized document objects</li> </ol> <p>Each parser creates documents with a common structure:</p> <pre><code>document = {\n    \"id\": hash_string(title),\n    \"title\": title,\n    \"description\": description,  # or \"abstract\" for XML\n    \"keywords\": keywords,\n    \"src\": \"gdd\"  # or \"rda\" or \"fsgeodata\"\n}\n</code></pre>"},{"location":"harvesting/#building-the-compiled-catalog","title":"Building the Compiled Catalog","text":"<p>After downloading and parsing from all three sources, <code>build_docs_catalog()</code> creates a single unified catalog file.</p> <p>What it does:</p> <pre><code>def build_docs_catalog():\n    # 1. Create instances of all three loaders\n    fsgeod = FSGeodataLoader()\n    rda = RDALoader()\n    gdd = GeospatialDataDiscovery()\n\n    # 2. Parse metadata from each source\n    fsgeo_docs = fsgeod.parse_metadata()\n    rda_docs = rda.parse_metadata()\n    gdd_docs = gdd.parse_metadata()\n\n    # 3. Combine all documents into one list\n    documents = fsgeo_docs + rda_docs + gdd_docs\n\n    # 4. Save to a single JSON file\n    save_json(documents, \"data/catalog.json\")\n</code></pre> <p>Output:</p> <ul> <li><code>data/catalog.json</code> - A single file containing all parsed documents from all three sources</li> </ul> <p>This compiled catalog serves as the input for loading into the vector database. Instead of reading hundreds of individual XML files and multiple JSON files, the vector database loader can simply read one consolidated file.</p> <p>Why this matters:</p> <ul> <li>Single source of truth: One file to maintain and version</li> <li>Faster loading: Vector DB can read one file instead of hundreds</li> <li>Easy inspection: You can examine the entire catalog structure in one place</li> </ul> <p>Running it:</p> <pre><code># Build the compiled catalog from all sources\ntimbercat build-docs-catalog\n</code></pre> <p>The command reports how many documents were parsed from each source and the total count.</p>"},{"location":"harvesting/#key-design-decisions","title":"Key Design Decisions","text":"<p>Source Identification: Each document includes a <code>src</code> field to track which system it came from.</p> <p>ID Generation: Document IDs are generated by hashing the title, ensuring uniqueness and consistency.</p> <p>Error Handling: All harvesters handle missing files and failed requests gracefully.</p> <p>Rate Limiting: FSGeodata includes a 0.5s delay between requests to be respectful of server resources.</p>"},{"location":"harvesting/#running-the-harvesters","title":"Running the Harvesters","text":"<pre><code># Download from Clearinghouse (XML + JSON)\ntimbercat download-fsgeodata\n\n# Other sources can be added to the CLI similarly\n</code></pre>"},{"location":"harvesting/#challenges","title":"Challenges","text":"<p>Different Data Formats: XML (FSGeodata) vs JSON (GDD, RDA) require different parsing strategies.</p> <p>Inconsistent Schemas: Not all XML metadata files have the same structure. Defensive parsing with existence checks is essential.</p> <p>Missing Services: Some FSGeodata datasets don't have MapServer services. The harvester handles these gracefully.</p>"},{"location":"harvesting/#next-steps","title":"Next Steps","text":"<p>After harvesting, the parsed documents are loaded into ChromaDB for semantic search. See the implementation in <code>src/catalog/</code> for the full loader classes.</p>"}]}