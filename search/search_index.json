{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Catalog","text":"<p>Catalog is a Python command-line-interface (CLI) that automates discovery and understanding of Forest Service geospatial and tabular data. It harvests XML metadata and MapServer service definitions from three anonymized portals\u2014Research Data Archive (RDA), Geospatial Data Discovery (GDD), and FSGeodata Clearinghouse (FSGeodata)\u2014builds embeddings, and allows the user to explore datasets with a semantic, RAG-powered search that can answer questions like \u201cWhat data exists and how do I use it?\u201d.</p>"},{"location":"#why-it-matters","title":"Why it matters","text":"<ul> <li>Hunting across portals, downloading XML one-by-one, and reconciling service URLs slows research and product teams.</li> <li>Explaining lineage and \u201cfit for purpose\u201d to stakeholders is hard without a unified view.</li> <li>Traditional keyword search misses nuance; semantic search with LLMs locates relevant datasets faster.</li> </ul>"},{"location":"#what-catalog-does","title":"What Catalog does","text":"<ul> <li>Automated harvesting from RDA, GDD, and FSGeodata (XML + MapServer JSON).</li> <li>Embeds metadata with a vector database (table) and uses LLMs in a Retrieval-Augmented Generation (RAG) flow for semantic Q&amp;A.</li> <li>Outputs organized metadata and service URLs you can plug into dashboards or analyses.</li> </ul>"},{"location":"#the-process-at-a-glance","title":"The process (at a glance)","text":"<pre><code>flowchart TB\n  Sources[RDA / GDD / FSGeodata] --&gt; Download\n  Download --&gt; Normalize[\"Normalize metadata (XML + JSON)\"]\n  Normalize --&gt; Embed[VectorDB embeddings]\n  Embed --&gt; RAG[LLM + RAG pipeline]\n  RAG --&gt; Answers[Semantic search &amp; dataset guidance]\n</code></pre> <ol> <li>Identify metadata sources:  </li> <li>Research Archive (RDA): research-grade datasets from the agency's research directorate and partners.  </li> <li>Geospatial Discovery (GDD): current operational GIS layers and services.  </li> <li> <p>FSGeodata Clearinghouse (FSGeodata): authoritative basemaps, boundaries, operational layers, and raster products.</p> </li> <li> <p>Harvest metadata: <code>uv run catalog download-fs-metadata</code> pulls XML and MapServer JSON, normalizes fields, and stores them for indexing.</p> </li> <li> <p>Build the vector database: embeddings go into vector storage; metadata stays linked for provenance.</p> </li> <li> <p>RAG-based search: the CLI uses the embeddings plus an LLM to answer dataset and lineage questions with grounded citations.</p> </li> </ol>"},{"location":"#where-to-go-next","title":"Where to go next","text":"<ul> <li>Architecture and design decisions: see <code>docs/architecture.md</code>.</li> <li>Data sources deep dive: see <code>docs/data-sources.md</code>.</li> <li>CLI usage and examples: see <code>docs/cli.md</code>.</li> <li>Vector DB details and comparisons: see <code>docs/vector-db.md</code>.</li> </ul>"},{"location":"article-outline/","title":"Journal Article Outline","text":""},{"location":"article-outline/#title-suggested","title":"Title (suggested)","text":"<p>\"A Retrieval-Augmented Generation Approach for Discovering Heterogeneous Federal Geospatial Metadata\"</p>"},{"location":"article-outline/#1-introduction","title":"1. Introduction","text":"<ul> <li>Problem: Federal geospatial data fragmented across repositories with incompatible schemas</li> <li>Motivation: Researchers struggle to discover relevant USFS datasets using keyword search</li> <li>Contribution: A RAG-based system that harmonizes metadata and enables semantic discovery</li> <li>Research questions:</li> <li>Can schema harmonization improve cross-repository search?</li> <li>Does vector-based semantic search outperform keyword matching for geospatial metadata?</li> </ul>"},{"location":"article-outline/#2-background-related-work","title":"2. Background &amp; Related Work","text":"<ul> <li>Geospatial metadata standards (ISO 19115, DCAT-US, FGDC)</li> <li>Federal data discovery challenges (data.gov limitations)</li> <li>RAG architectures for information retrieval</li> <li>Vector databases for document search</li> </ul>"},{"location":"article-outline/#3-data-sources","title":"3. Data Sources","text":"<ul> <li>FSGeodata Clearinghouse (EDW) - XML/FGDC format</li> <li>Geospatial Data Discovery (GDD) - DCAT-US 1.1 JSON via ArcGIS Hub</li> <li>Research Data Archive (RDA) - Custom JSON API</li> <li>Characterization of each source's schema, coverage, and limitations</li> </ul>"},{"location":"article-outline/#4-methods","title":"4. Methods","text":"<ul> <li>Schema harmonization approach</li> <li>Vector embedding and indexing pipeline</li> <li>RAG architecture with LLM integration</li> <li>System implementation</li> </ul> <p>See <code>methods-section.md</code> for the detailed draft.</p>"},{"location":"article-outline/#5-results","title":"5. Results","text":"<ul> <li>Catalog statistics (document counts, field coverage)</li> <li>Query evaluation (semantic vs keyword search examples)</li> <li>User study or expert evaluation (if applicable)</li> </ul>"},{"location":"article-outline/#suggested-analyses","title":"Suggested Analyses","text":"<ul> <li>Table: Document counts by source</li> <li>Table: Field completeness rates across sources</li> <li>Figure: Query response comparison (keyword vs semantic)</li> <li>Example queries demonstrating semantic understanding</li> </ul>"},{"location":"article-outline/#6-discussion","title":"6. Discussion","text":"<ul> <li>Implications for federal data discovery</li> <li>Limitations:</li> <li>Embedding model choices and their impact on retrieval quality</li> <li>Information loss during schema harmonization</li> <li>Dependency on source API stability</li> <li>Generalizability to other federal repositories (EPA, NOAA, USGS)</li> </ul>"},{"location":"article-outline/#7-conclusion-future-work","title":"7. Conclusion &amp; Future Work","text":"<ul> <li>Summary of contributions</li> <li>Extensions:</li> <li>Additional data sources</li> <li>Fine-tuned embedding models for geospatial terminology</li> <li>Quantitative evaluation metrics (precision, recall, NDCG)</li> <li>User interface development</li> </ul>"},{"location":"article-outline/#target-journals-suggestions","title":"Target Journals (suggestions)","text":"<ul> <li>Computers &amp; Geosciences</li> <li>International Journal of Geographical Information Science</li> <li>Environmental Modelling &amp; Software</li> <li>Journal of the Association for Information Science and Technology (JASIST)</li> <li>Earth Science Informatics</li> </ul>"},{"location":"methods-section/","title":"Methods Section Draft","text":""},{"location":"methods-section/#3-methods","title":"3. Methods","text":""},{"location":"methods-section/#31-data-sources-and-collection","title":"3.1 Data Sources and Collection","text":"<p>We developed automated harvesters for three USFS geospatial data repositories, each employing distinct metadata standards and access mechanisms.</p> <p>FSGeodata Clearinghouse. The Enterprise Data Warehouse (EDW) datasets are accessed via the USFS Geodata portal (https://data.fs.usda.gov/geodata/edw/datasets.php). We implemented a web scraper using BeautifulSoup to parse the datasets index page, extracting links to XML metadata files conforming to FGDC Content Standard for Digital Geospatial Metadata. For each dataset, we retrieve both the metadata XML and, where available, associated ArcGIS MapServer service descriptors in JSON format. A rate limiter (250ms delay) ensures compliance with server policies.</p> <p>Geospatial Data Discovery (GDD). The USFS ArcGIS Hub portal exposes a DCAT-US 1.1 compliant feed at a single JSON endpoint (https://data-usfs.hub.arcgis.com/api/feed/dcat-us/1.1.json). This feed provides dataset titles, descriptions, keywords, and thematic classifications in a standardized federal open data format.</p> <p>Research Data Archive (RDA). The USFS Research Data Archive provides a JSON web service (https://www.fs.usda.gov/rds/archive/webservice/datagov) returning dataset metadata including titles, descriptions, and keyword arrays. This source emphasizes research datasets with scientific provenance.</p>"},{"location":"methods-section/#32-schema-harmonization","title":"3.2 Schema Harmonization","text":"<p>To enable cross-repository search, we defined a unified document schema (<code>USFSDocument</code>) with the following fields:</p> Field Type Description <code>id</code> string SHA-256 hash of normalized title (lowercase, trimmed) <code>title</code> string Dataset title <code>abstract</code> string Summary description (mapped from FGDC abstract or DCAT description) <code>purpose</code> string Intended use statement (FSGeodata only) <code>description</code> string General description text <code>keywords</code> array Subject keywords (from themekey, keyword, or theme fields) <code>src</code> string Source identifier: \"fsgeodata\", \"gdd\", or \"rda\" <code>lineage</code> array Processing history with dates (FSGeodata only) <p>The <code>id</code> field serves as a deduplication key, ensuring datasets appearing in multiple repositories are not double-counted. Text fields undergo normalization including whitespace collapsing and Unicode standardization via a <code>clean_str()</code> utility function.</p> <p>Schema Mapping. Each source requires distinct parsing logic:</p> <ul> <li>FSGeodata: XML parsing extracts <code>&lt;title&gt;</code>, <code>&lt;descript&gt;&lt;abstract&gt;</code>, <code>&lt;descript&gt;&lt;purpose&gt;</code>, <code>&lt;themekey&gt;</code>, and <code>&lt;procstep&gt;</code> elements</li> <li>GDD: JSON mapping from DCAT fields (<code>title</code>, <code>description</code>, <code>keyword</code>, <code>theme</code>)</li> <li>RDA: Direct JSON field extraction (<code>title</code>, <code>description</code>, <code>keyword</code>)</li> </ul>"},{"location":"methods-section/#33-vector-embedding-and-storage","title":"3.3 Vector Embedding and Storage","text":"<p>Harmonized documents are loaded into ChromaDB, an open-source vector database. For each document, we construct an embedding input string concatenating:</p> <pre><code>Title: {title}\nAbstract: {abstract}\nPurpose: {purpose}\nSource: {src}\nKeywords: {keywords}\nLineage: {lineage}\n</code></pre> <p>ChromaDB's default embedding model generates vector representations stored alongside document metadata. Documents are processed in batches of 100 to optimize memory usage. The collection is rebuilt from scratch on each indexing operation to ensure consistency.</p>"},{"location":"methods-section/#34-retrieval-augmented-generation","title":"3.4 Retrieval-Augmented Generation","text":"<p>The system supports two query modes:</p> <p>Vector Search. Users submit natural language queries, which are embedded and compared against the document collection using cosine distance. The top-k results (configurable, default k=5) are returned with relevance distances, where lower distances indicate higher semantic similarity.</p> <p>LLM-Augmented Discovery. For complex discovery questions, we implement a RAG pipeline:</p> <ol> <li>The user query is used to retrieve the top-k relevant documents via vector search</li> <li>Retrieved documents are formatted as context</li> <li>The query and context are submitted to an LLM (configurable via Ollama API)</li> <li>The LLM generates a natural language response synthesizing the search results</li> </ol> <p>The LLM system prompt frames the model as a \"data librarian\" with instructions to:</p> <ul> <li>List relevant datasets with explanations of relevance</li> <li>Prioritize results by distance score (lower = more relevant)</li> <li>Provide direct yes/no answers for existence queries</li> <li>Avoid speculation beyond catalog contents</li> </ul>"},{"location":"methods-section/#35-implementation","title":"3.5 Implementation","text":"<p>The system is implemented in Python as a CLI tool using the Click framework. Key dependencies include:</p> <ul> <li>ChromaDB for vector storage and similarity search</li> <li>Ollama client for LLM integration</li> <li>BeautifulSoup for XML/HTML parsing</li> <li>Pydantic for schema validation</li> <li>Requests for HTTP operations</li> </ul> <p>The modular architecture separates concerns: data loaders (<code>usfs.py</code>), schema definitions (<code>schema.py</code>), vector operations (<code>core.py</code>), and LLM integration (<code>bots.py</code>).</p>"},{"location":"methods-section/#36-system-architecture","title":"3.6 System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Data Collection                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   FSGeodata     \u2502        GDD          \u2502          RDA            \u2502\n\u2502   (XML/FGDC)    \u2502    (DCAT-US 1.1)    \u2502        (JSON)           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                   \u2502                       \u2502\n         \u25bc                   \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Schema Harmonization                         \u2502\n\u2502                      (USFSDocument)                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Unified Catalog (JSON)                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Vector Embedding &amp; Indexing                     \u2502\n\u2502                        (ChromaDB)                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u25bc                               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Vector Search       \u2502     \u2502      LLM-Augmented Discovery    \u2502\n\u2502   (Semantic Queries)    \u2502     \u2502     (Natural Language Q&amp;A)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"methods-section/#37-reproducibility","title":"3.7 Reproducibility","text":"<p>The complete source code is available at [repository URL]. To reproduce the catalog:</p> <pre><code># Install dependencies\npip install -e .\n\n# Download metadata from all sources\ncatalog download_fs_metadata\n\n# Build unified catalog\ncatalog build_fs_catalog\n\n# Index into vector database\ncatalog build_fs_chromadb\n\n# Query the catalog\ncatalog query_fs_chromadb -q \"forest fire data\" -n 5\n</code></pre> <p>Environment configuration requires setting <code>OLLAMA_API_KEY</code>, <code>OLLAMA_API_URL</code>, and <code>OLLAMA_MODEL</code> for LLM-augmented queries.</p>"}]}